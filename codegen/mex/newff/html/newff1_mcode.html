<!-- saved from url=(0014)about:internet -->
<pre class="code">
<span class="srcline"><span class="lineno"><a href="1,1" id="srcline1">  1</a></span><span class="line"><span class="keyword">function</span> <span class="var type0" id="S2T0U3">out1</span> = <span class="message fatal" id="M1F1C">newff</span>(<span class="var type0" id="S3T0U6">varargin</span>)</span></span>
<span class="srcline"><span class="lineno"><a href="1,2" id="srcline2">  2</a></span><span class="line"><span class="comment">%NEWFF Create a feed-forward backpropagation network.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,3" id="srcline3">  3</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,4" id="srcline4">  4</a></span><span class="line"><span class="comment">%  Obsoleted in R2010b NNET 7.0.  Last used in R2010a NNET 6.0.4.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,5" id="srcline5">  5</a></span><span class="line"><span class="comment">%  The recommended function is &lt;a href="matlab:doc feedforwardnet"&gt;feedforwardnet&lt;/a&gt;.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,6" id="srcline6">  6</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,7" id="srcline7">  7</a></span><span class="line"><span class="comment">%  Syntax</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,8" id="srcline8">  8</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,9" id="srcline9">  9</a></span><span class="line"><span class="comment">%    net = newff(P,T,S)</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,10" id="srcline10"> 10</a></span><span class="line"><span class="comment">%    net = newff(P,T,S,TF,BTF,BLF,PF,IPF,OPF,DDF)</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,11" id="srcline11"> 11</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,12" id="srcline12"> 12</a></span><span class="line"><span class="comment">%  Description</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,13" id="srcline13"> 13</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,14" id="srcline14"> 14</a></span><span class="line"><span class="comment">%    NEWFF(P,T,S) takes,</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,15" id="srcline15"> 15</a></span><span class="line"><span class="comment">%      P  - RxQ1 matrix of Q1 representative R-element input vectors.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,16" id="srcline16"> 16</a></span><span class="line"><span class="comment">%      T  - SNxQ2 matrix of Q2 representative SN-element target vectors.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,17" id="srcline17"> 17</a></span><span class="line"><span class="comment">%      Si  - Sizes of N-1 hidden layers, S1 to S(N-1), default = [].</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,18" id="srcline18"> 18</a></span><span class="line"><span class="comment">%            (Output layer size SN is determined from T.)</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,19" id="srcline19"> 19</a></span><span class="line"><span class="comment">%    and returns an N layer feed-forward backprop network.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,20" id="srcline20"> 20</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,21" id="srcline21"> 21</a></span><span class="line"><span class="comment">%    NEWFF(P,T,S,TF,BTF,BLF,PF,IPF,OPF,DDF) takes optional inputs,</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,22" id="srcline22"> 22</a></span><span class="line"><span class="comment">%      TFi - Transfer function of ith layer. Default is 'tansig' for</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,23" id="srcline23"> 23</a></span><span class="line"><span class="comment">%            hidden layers, and 'purelin' for output layer.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,24" id="srcline24"> 24</a></span><span class="line"><span class="comment">%      BTF - Backprop network training function, default = 'trainlm'.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,25" id="srcline25"> 25</a></span><span class="line"><span class="comment">%      BLF - Backprop weight/bias learning function, default = 'learngdm'.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,26" id="srcline26"> 26</a></span><span class="line"><span class="comment">%      PF  - Performance function, default = 'mse'.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,27" id="srcline27"> 27</a></span><span class="line"><span class="comment">%      IPF - Row cell array of input processing functions.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,28" id="srcline28"> 28</a></span><span class="line"><span class="comment">%            Default is {'fixunknowns','remconstantrows','mapminmax'}.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,29" id="srcline29"> 29</a></span><span class="line"><span class="comment">%      OPF - Row cell array of output processing functions.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,30" id="srcline30"> 30</a></span><span class="line"><span class="comment">%            Default is {'remconstantrows','mapminmax'}.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,31" id="srcline31"> 31</a></span><span class="line"><span class="comment">%      DDF - Data division function, default = 'dividerand';</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,32" id="srcline32"> 32</a></span><span class="line"><span class="comment">%    and returns an N layer feed-forward backprop network.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,33" id="srcline33"> 33</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,34" id="srcline34"> 34</a></span><span class="line"><span class="comment">%    The transfer functions TF{i} can be any differentiable transfer</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,35" id="srcline35"> 35</a></span><span class="line"><span class="comment">%    function such as TANSIG, LOGSIG, or PURELIN.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,36" id="srcline36"> 36</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,37" id="srcline37"> 37</a></span><span class="line"><span class="comment">%    The training function BTF can be any of the backprop training</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,38" id="srcline38"> 38</a></span><span class="line"><span class="comment">%    functions such as TRAINLM, TRAINBFG, TRAINRP, TRAINGD, etc.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,39" id="srcline39"> 39</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,40" id="srcline40"> 40</a></span><span class="line"><span class="comment">%    *WARNING*: TRAINLM is the default training function because it</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,41" id="srcline41"> 41</a></span><span class="line"><span class="comment">%    is very fast, but it requires a lot of memory to run.  If you get</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,42" id="srcline42"> 42</a></span><span class="line"><span class="comment">%    an "out-of-memory" error when training try doing one of these:</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,43" id="srcline43"> 43</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,44" id="srcline44"> 44</a></span><span class="line"><span class="comment">%    (1) Slow TRAINLM training, but reduce memory requirements, by</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,45" id="srcline45"> 45</a></span><span class="line"><span class="comment">%        setting NET.&lt;a href="matlab:doc nnproperty.net_efficiency"&gt;efficiency&lt;/a&gt;.&lt;a href="matlab:doc nnproperty.net_efficiency_memoryReduction"&gt;memoryReduction&lt;/a&gt; to 2 or more. (See HELP TRAINLM.)</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,46" id="srcline46"> 46</a></span><span class="line"><span class="comment">%    (2) Use TRAINBFG, which is slower but more memory efficient than TRAINLM.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,47" id="srcline47"> 47</a></span><span class="line"><span class="comment">%    (3) Use TRAINRP which is slower but more memory efficient than TRAINBFG.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,48" id="srcline48"> 48</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,49" id="srcline49"> 49</a></span><span class="line"><span class="comment">%    The learning function BLF can be either of the backpropagation</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,50" id="srcline50"> 50</a></span><span class="line"><span class="comment">%    learning functions such as LEARNGD, or LEARNGDM.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,51" id="srcline51"> 51</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,52" id="srcline52"> 52</a></span><span class="line"><span class="comment">%    The performance function can be any of the differentiable performance</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,53" id="srcline53"> 53</a></span><span class="line"><span class="comment">%    functions such as MSE or MSEREG.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,54" id="srcline54"> 54</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,55" id="srcline55"> 55</a></span><span class="line"><span class="comment">%  Examples</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,56" id="srcline56"> 56</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,57" id="srcline57"> 57</a></span><span class="line"><span class="comment">%    [inputs,targets] = simplefitdata;</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,58" id="srcline58"> 58</a></span><span class="line"><span class="comment">%    net = newff(inputs,targets,20);</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,59" id="srcline59"> 59</a></span><span class="line"><span class="comment">%    net = train(net,inputs,targets);</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,60" id="srcline60"> 60</a></span><span class="line"><span class="comment">%    outputs = net(inputs);</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,61" id="srcline61"> 61</a></span><span class="line"><span class="comment">%    errors = outputs - targets;</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,62" id="srcline62"> 62</a></span><span class="line"><span class="comment">%    perf = perform(net,outputs,targets)</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,63" id="srcline63"> 63</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,64" id="srcline64"> 64</a></span><span class="line"><span class="comment">%  Algorithm</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,65" id="srcline65"> 65</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,66" id="srcline66"> 66</a></span><span class="line"><span class="comment">%    Feed-forward networks consist of Nl layers using the DOTPROD</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,67" id="srcline67"> 67</a></span><span class="line"><span class="comment">%    weight function, NETSUM net input function, and the specified</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,68" id="srcline68"> 68</a></span><span class="line"><span class="comment">%    transfer functions.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,69" id="srcline69"> 69</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,70" id="srcline70"> 70</a></span><span class="line"><span class="comment">%    The first layer has weights coming from the input.  Each subsequent</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,71" id="srcline71"> 71</a></span><span class="line"><span class="comment">%    layer has a weight coming from the previous layer.  All layers</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,72" id="srcline72"> 72</a></span><span class="line"><span class="comment">%    have biases.  The last layer is the network output.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,73" id="srcline73"> 73</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,74" id="srcline74"> 74</a></span><span class="line"><span class="comment">%    Each layer's weights and biases are initialized with INITNW.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,75" id="srcline75"> 75</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,76" id="srcline76"> 76</a></span><span class="line"><span class="comment">%    Adaption is done with TRAINS which updates weights with the</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,77" id="srcline77"> 77</a></span><span class="line"><span class="comment">%    specified learning function. Training is done with the specified</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,78" id="srcline78"> 78</a></span><span class="line"><span class="comment">%    training function. Performance is measured according to the specified</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,79" id="srcline79"> 79</a></span><span class="line"><span class="comment">%    performance function.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,80" id="srcline80"> 80</a></span><span class="line"><span class="comment">%</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,81" id="srcline81"> 81</a></span><span class="line"><span class="comment">%  See also NEWCF, NEWELM, SIM, INIT, ADAPT, TRAIN, TRAINS</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,82" id="srcline82"> 82</a></span><span class="line"></span></span>
<span class="srcline"><span class="lineno"><a href="1,83" id="srcline83"> 83</a></span><span class="line"><span class="comment">% Mark Beale, 11-31-97</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,84" id="srcline84"> 84</a></span><span class="line"><span class="comment">% Copyright 1992-2010 The MathWorks, Inc.</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,85" id="srcline85"> 85</a></span><span class="line"></span></span>
<span class="srcline"><span class="lineno"><a href="1,86" id="srcline86"> 86</a></span><span class="line"><span class="comment">%disp('NEWFF is no longer recommended. FEEDFORWARD is simpler and more efficient.');</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,87" id="srcline87"> 87</a></span><span class="line"><span class="comment">% TODO - Recommendation function NNRECOMMEND</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,88" id="srcline88"> 88</a></span><span class="line"></span></span>
<span class="srcline"><span class="lineno"><a href="1,89" id="srcline89"> 89</a></span><span class="line"><span class="comment">%% Boilerplate Code - Same for all Network Functions</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,90" id="srcline90"> 90</a></span><span class="line"></span></span>
<span class="srcline"><span class="lineno"><a href="1,91" id="srcline91"> 91</a></span><span class="line"><span class="keyword">persistent</span> <span class="var type0" id="S4T0U8">INFO</span>;</span></span>
<span class="srcline"><span class="lineno"><a href="1,92" id="srcline92"> 92</a></span><span class="line"><span class="keyword">if</span> (nargin &lt; 1), error(message(<span class="string">'nnet:Args:NotEnough'</span>)); <span class="keyword">end</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,93" id="srcline93"> 93</a></span><span class="line"><span class="var type0" id="S8T0U24">in1</span> = <span class="var type0" id="S3T0U26">varargin</span>{1};</span></span>
<span class="srcline"><span class="lineno"><a href="1,94" id="srcline94"> 94</a></span><span class="line"><span class="keyword">if</span> ischar(<span class="var type0" id="S8T0U32">in1</span>)</span></span>
<span class="srcline"><span class="lineno"><a href="1,95" id="srcline95"> 95</a></span><span class="line">  <span class="keyword">switch</span> <span class="var type0" id="S8T0U34">in1</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,96" id="srcline96"> 96</a></span><span class="line">    <span class="keyword">case</span> <span class="string">'info'</span>,</span></span>
<span class="srcline"><span class="lineno"><a href="1,97" id="srcline97"> 97</a></span><span class="line">      <span class="keyword">if</span> isempty(<span class="var type0" id="S4T0U41">INFO</span>), <span class="var type0" id="S4T0U44">INFO</span> = get_info; <span class="keyword">end</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,98" id="srcline98"> 98</a></span><span class="line">      <span class="var type0" id="S2T0U49">out1</span> = <span class="var type0" id="S4T0U50">INFO</span>;</span></span>
<span class="srcline"><span class="lineno"><a href="1,99" id="srcline99"> 99</a></span><span class="line">  <span class="keyword">end</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,100" id="srcline100">100</a></span><span class="line"><span class="keyword">else</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,101" id="srcline101">101</a></span><span class="line">  <span class="var type0" id="S2T0U54">out1</span> = create_network(<span class="var type0" id="S3T0U58">varargin</span>{:});</span></span>
<span class="srcline"><span class="lineno"><a href="1,102" id="srcline102">102</a></span><span class="line"><span class="keyword"><span class="keyword">end</span></span></span></span>
<span class="srcline"><span class="lineno"><a href="1,103" id="srcline103">103</a></span><span class="line"></span></span>
<span class="srcline"><span class="lineno"><a href="1,104" id="srcline104">104</a></span><span class="line"><span class="comment">%% Boilerplate Code - Same for all Network Functions</span></span></span>
<span class="srcline"><span class="lineno"><a href="1,105" id="srcline105">105</a></span><span class="line"></span></span>
<span class="srcline"><span class="lineno"><a href="1,106" id="srcline106">106</a></span><span class="line"><span class="comment">%%</span></span></span>
</pre>
